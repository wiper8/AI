% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/DDPG.R
\name{DDPG}
\alias{DDPG}
\title{Train a DDPG system.}
\usage{
DDPG(
  policy_nn,
  critic_nn,
  actualize,
  reset,
  reward,
  done,
  limit_FUN,
  episodes,
  buffer_len,
  batch_size,
  explor,
  gradient_step_n,
  discount,
  polyak,
  object_inputs,
  see,
  track_weights = FALSE,
  ...
)
}
\arguments{
\item{policy_nn}{policy nn to choose actions.}

\item{critic_nn}{critic nn to predict reward.}

\item{actualize}{function to move object.}

\item{reset}{function to reset object.}

\item{reward}{function to get reward.}

\item{done}{function to determine if episode if done.}

\item{episodes}{integer : number of scenarios to train to.}

\item{buffer_len}{integer : length of the replay buffer.}

\item{batch_size}{integer : length of the batches used to backpropagate networks.}

\item{explor}{numeric : >0 standard deviation of a value added to all weights in policy.}

\item{gradient_step_n}{integer : number of backpropagation steps before moving on to updating targets.}

\item{discount}{numeric : [0, 1] actualisation rate.}

\item{polyak}{numeric : [0, 1] percentage of the new targets that we keep to update the actual targets.}

\item{object_inputs}{function : function to get together in a data.frame the inputs for policy.}

\item{see}{function : function to see the agent do his actions.}

\item{track_weights}{logical : if the evolution of weights will be used in a graph.}

\item{...}{(optional) other arguments passed to other functions.}
}
\value{
list of the policy and the critic's weights (and the tracking of the weights if track_weights is TRUE).
}
\description{
Train a DDPG system.
}
\examples{
DDPG(policy_nn=neuralnetwork(acc+steer~l+t_l+t+t_r+r+speed, c(3, 2)),
critic_nn=neuralnetwork(reward~l+t_l+t+t_r+r+speed+acc+steer, c(4, 3)),
actualize=car::actualize,
reset=function(...) car:::radar_distance(car::set(car::reset(x=0, y=0, orien=90)), walls=car::walls),
reward=car::reward,
done=car::done,
limit_FUN=car::limits_FUN,
episodes = 1,
buffer_len = 10,
batch_size = 4,
explor = 0.1,
gradient_step_n = 5,
discount = 0.999,
polyak = 0.9,
object_inputs=function(x) cbind(x$dist, speed=x$speed),
see=car::see_car_track,
track_weights=TRUE,
t=0.1,
walls=car::walls,
finish=car::finish
)
}
