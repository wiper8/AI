% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/DDPG.R
\name{DDPG}
\alias{DDPG}
\title{Train a DDPG system.}
\usage{
DDPG(
  policy_nn,
  critic_nn,
  actualize,
  reset,
  reward,
  done,
  episodes,
  buffer_len,
  batch_size,
  explor,
  gradient_step_n,
  discount,
  polyak,
  object_inputs,
  see,
  track_weights = FALSE,
  track_object = FALSE,
  ...
)
}
\arguments{
\item{policy_nn}{policy nn to choose actions.}

\item{critic_nn}{critic nn to predict reward.}

\item{actualize}{function to move object.}

\item{reset}{function to reset object.}

\item{reward}{function to get reward.}

\item{done}{function to determine if episode if done.}

\item{episodes}{integer : number of scenarios to train to.}

\item{buffer_len}{integer : length of the replay buffer.}

\item{batch_size}{integer : length of the batches used to backpropagate networks.}

\item{explor}{numeric : >0 standard deviation of a value added to all weights in policy.}

\item{gradient_step_n}{integer : number of backpropagation steps before moving on to updating targets.}

\item{discount}{numeric : [0, 1] actualisation rate.}

\item{polyak}{numeric : [0, 1] percentage of the new targets that we keep to update the actual targets.}

\item{object_inputs}{function : function to get together in a data.frame the inputs for policy.}

\item{see}{function : function to see the agent do his actions.}

\item{track_weights}{logical : if the evolution of weights will be used in a graph.}

\item{track_object}{logical : if the past states of object are to be seen.}

\item{...}{(optional) other arguments passed to other functions.}

\item{max_iter}{integer : maximum number of iterations for every episode.}
}
\value{
list of the policy and the critic's weights (and the tracking of the weights if track_weights is TRUE) and a plot of the last car position/line.
}
\description{
Train a DDPG system.
}
